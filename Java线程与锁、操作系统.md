# Java线程与锁、操作系统

[toc]

# Java线程与锁

### 1.线程wait(),sleep():

wait()和sleep()的关键的区别在于，wait()是用于线程间通信的，而sleep()是用于短时间暂停当前线程。更加明显的一个区别在于，当一个线程调用wait()方法的时候，会释放它锁持有的对象的管程和锁，但是调用sleep()方法的时候，不会释放他所持有的管程。
1.所属的类不同：
sleep方法是定义在Thread上
wait方法是定义在Object上
2.对于锁资源的处理方式不同
sleep不会释放锁
wait会释放锁
3.使用范围：
sleep可以使用在任何代码块
wait必须在同步方法或同步代码块执行
4.唤醒：
void notify()可以唤醒等待（wait）的单个线程，而sleep不能被唤醒

线程sleep()，yield():
1. 都会暂缓执行当前线程；
2. 如果已经持有锁，那么在等待过程中都不会释放锁；

不同点在于：
1. Thread.sleep()可以精确指定休眠的时间，而Thread.yield()依赖于CPU的时间片划分，在我的电脑上大约为20微秒；

2. Thread.sleep()会抛出中断异常，且能被中断，而Thread.yield()不可以；
    yield()理解为线程让步
    yeild()只是让当前线程暂停一下，让系统的线程调度器重新调度一次，完全可能的情况是：当某个线程调用了yield()线程暂停之后，线程调度器又将其调度出来重新执行。当某个线程调用了yield()方法暂停之后，只有优先级与当前线程相同，或者优先级比当前线程更高的处于就绪状态的线程才会获得执行机会。

  

### 2.线程生命周期

1.当线程调用wait()或者join()时，线程都会进入到waiting（等待）状态，当调用notify或notifyAll时，或者join的线程执行结束后，会进入runnable状态
2.当线程调用sleep(time)，或者wait(time)时，进入timed waiting状态
join：可理解为插队，对当前线程是一种特殊的wait。当前运行线程调用另一个线程的join方法，当前线程进入阻塞状态直到被调用线程运行结束，之后当前线程继续。 注意该方法也需要捕捉异常。等待调用join方法的线程结束，再继续执行。如在main()中调用thread.join()，意为阻塞main线程并调用thread线程，thread线程运行结束之后main线程继续


Thread.interrupt()方法不会中断一个正在运行的线程。它的作用是，在线程受到阻塞时抛出一个中断信号，这样线程就得以退出阻塞的状态。更确切的说，如果线程被Object.wait,Thread.join和Thread.sleep三种方法之一阻塞，那么，它将接收到一个中断异常（InterruptedException），从而提早地终结被阻塞状态。

interrupt方法并不是强制终止线程，它只能设置线程的interrupted状态（调用后设置为true），而在线程中一般使用一下方式：
while (!Thread.currentThread().isInterrupted() && more work to do)，来结束run方法



### 3.线程池

#### 3.1 新建线程池：

1.Excutors.newFixedThreadPool(int)
创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。newFixedThreadPool固定线程池，  使用完毕必须手动关闭线程池，否则会一直在内存中存在。

2.Excutors.newSingleThreadExecutor()
创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序执行。newSingleThreadExecutor创建的线程池corePoolSize和maximumPoolSize值都设置为1，它使用的LinkedBlockingQueue。

3.Excutors.newCachedThreadPool()
创建一个可缓存线程池，如果线程池长度超出处理需要，可灵活回收空闲线程，若无可回收，则新建线程。
newCachedThreadPool将corePoolSize设置为0，将maximumPoolSize设置为Integer.MAX_VALUE，使用的SynchronousQueue，多余的空闲线程的存活时间60秒。

4.Excutors.newScheduledThreadPool(int)
创建一个周期性的线程池，支持定时及周期性执行任务。



#### 3.2 线程池7大参数：

1.corePoolSize(int)：核心线程数量。默认情况下，在创建了线程池后，线程池中的线程数为0，当有任务来之后，就会创建一个线程去执行任务，当线程池中的线程数目达到corePoolSize后，就会把到达的任务放到任务队列当中。线程池将长期保证这些线程处于存活状态，即使线程已经处于闲置状态。除非配置了allowCoreThreadTimeOut=true，核心线程数的线程也将不再保证长期存活于线程池内，在空闲时间超过keepAliveTime后被销毁。

2.workQueue：阻塞队列，存放等待执行的任务，线程从workQueue中取任务，若无任务将阻塞等待。当线程池中线程数量达到corePoolSize后，就会把新任务放到该队列当中。JDK提供了四个可直接使用的队列实现，分别是：基于数组的有界队列ArrayBlockingQueue、基于链表的无界队列LinkedBlockingQueue、只有一个元素的同步队列SynchronousQueue、优先级队列PriorityBlockingQueue。在实际使用时一定要设置队列长度。

3.maximumPoolSize(int)：线程池内的最大线程数量，线程池内维护的线程不得超过该数量，大于核心线程数量小于最大线程数量的线程将在空闲时间超过keepAliveTime后被销毁。当阻塞队列存满后，将会创建新线程执行任务，线程的数量不会大于maximumPoolSize。

4.keepAliveTime(long)：线程存活时间，若线程数超过了corePoolSize，线程闲置时间超过了存活时间，该线程将被销毁。除非配置了allowCoreThreadTimeOut=true，核心线程数的线程也将不再保证长期存活于线程池内，在空闲时间超过keepAliveTime后被销毁。

5.unit：keepAliveTime的单位

6.threadFactory：创建线程的工厂，虽说JDK提供了线程工厂的默认实现DefaultThreadFactory，但还是建议自定义实现最好，这样可以自定义线程创建的过程，例如线程分组、自定义线程名称等。

7.handler：拒绝策略

当工作队列中的任务已到达最大限制，并且线程池中的线程数量也达到最大限制（maximumPoolSize），这时如果有新任务提交进来，如何来拒绝请求执行的runnable的策略
①CallerRunsPolicy
该策略下，使用调用者所在线程来运行任务。在调用者线程中直接执行被拒绝任务的run方法，除非线程池已shutdown，则直接抛弃任务。
②AbortPolicy
该策略下，直接丢弃任务，并抛出RejectedExecutionException异常。
③DiscardPolicy
该策略下，直接丢弃任务，什么都不做。
④DiscardOldestPolicy
该策略下，抛弃进入队列最早的那个任务，然后尝试把这次拒绝的任务放入队列



### 4.threadlocal:

threadlocal里存储ThreadLoalMap。ThreadLocalMap是threadlocal里的静态内部类
ThreadLocal的实现是这样的：每个Thread 维护一个 ThreadLocalMap 映射表，这个映射表的 key是 ThreadLocal 实例本身，value是真正需要存储的 Object（同线程每次new一个ThreadLocal，map桶被占用+1）。
也就是说 ThreadLocal 本身并不存储值，它只是作为一个 key 来让线程从 ThreadLocalMap 获取 value。值得注意的是图中的虚线，表示 ThreadLocalMap 是使用 ThreadLocal 的弱引用作为 Key的，弱引用的对象在 GC 时会被回收。



ThreadLocal为什么会内存泄漏：
ThreadLocalMap的key为ThreadLocal对象的弱引用，我们知道弱引用有利于GC的回收，当key == null时，GC就会回收这部分空间，但value不一定能被回收，因为他和Current Thread之间还存在一个强引用的关系。
由于这个强引用的关系，会导致value无法回收，如果线程对象不消除这个强引用的关系，就可能会出现OOM。有些时候我们调用ThreadLocalMap的remove()方法进行显式处理。

在ThreadLoalMap中，也是初始化一个大小16的Entry数组，Entry对象用来保存每一个key-value键值对，只不过这里的key永远都是ThreadLocal对象，通过ThreadLocal对象的set方法，结果把ThreadLocal对象自己当做key，放进了ThreadLoalMap中。当执行get方法中，是从当前线程的threadLocals变量获取.
ThreadLoalMap的Entry是继承WeakReference，和HashMap很大的区别是，Entry中没有next字段，所以就不存在链表的情况了。

HashMap:底层为Node实现了Entry接口
static class Node<K,V> implements Map.Entry<K,V> {
        final int hash;
        final K key;
        V value;
        Node<K,V> next;

        Node(int hash, K key, V value, Node<K,V> next) {
            this.hash = hash;
            this.key = key;
            this.value = value;
            this.next = next;
        }......



### 5.volatile关键字：

不保证原子性，保证可见性与有序性
保证线程间变量的可见性。
禁止CPU进行指令重排序。

volatile实际也是JVM通过加内存屏障实现禁止指令重排

JVM指令重排：
在虚拟机层面，为了尽可能减少内存操作速度远慢于CPU运行速度所带来的CPU空置的影响，虚拟机会按照自己的一些规则(这规则后面再叙述)将程序编写顺序打乱——即写在后面的代码在时间顺序上可能会先执行，而写在前面的代码会后执行——以尽可能充分地利用CPU。拿上面的例子来说：假如不是a=1的操作，而是a=new byte1024*1024，那么它会运行地很慢，此时CPU是等待其执行结束呢，还是先执行下面那句flag=true呢？显然，先执行flag=true可以提前使用CPU，加快整体效率，当然这样的前提是不会产生错误(什么样的错误后面再说)。虽然这里有两种情况：后面的代码先于前面的代码开始执行；前面的代码先开始执行，但当效率较慢的时候，后面的代码开始执行并先于前面的代码执行结束。不管谁先开始，总之后面的代码在一些情况下存在先结束的可能。


禁止指令重排：
被volatile修饰的变量，会加一个lock前缀的汇编指令（lock cmpxchg）。若变量被修改后，会立刻将变量由工作内存回写到主存中。那么意味了之前的操作已经执行完毕。这就是内存屏障。它确保指令重排序时不会把其后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障的后面；即在执行到内存屏障这句指令时，在它前面的操作已经全部完成。非常经典的例子是在单例方法中同时对字段加入voliate，就是为了防止指令重排序。



### 6.Lock接口的实现类：Reentrantlock

private final Reentrantlock lockname = new Reentrantlock();
#### 6.1 Reentrantlock为显示定义的加锁
try{
   lockname.lock();
   ...
   ...
}
finally {
   lockname.unlock();
}

#### 6.2 ReentrantLock与公平锁、非公平锁实现
//创建一个非公平锁，默认是非公平锁
Lock lock = new ReentrantLock();
Lock lock = new ReentrantLock(false);

//创建一个公平锁，构造传参true
Lock lock = new ReentrantLock(true);

所谓的公平与非公平指的是在请求先后顺序上，先对锁进行请求的就一定先获取到锁，那么这就是公平锁，反之，如果对于锁的获取并没有时间上的先后顺序，如后请求的线程可能先获取到锁，这就是非公平锁，一般而言非，非公平锁机制的效率往往会胜过公平锁的机制，但在某些场景下，可能更注重时间先后顺序，那么公平锁自然是很好的选择。



### 7.synchronized

synchronized可重入锁的实现原理：
synchronized底层的实现原理是利用计算机系统的mutex Lock实现。每一个可重入锁都会关联一个线程ID和一个锁状态status。
当一个线程请求方法时，会去检查锁状态，如果锁状态是0，代表该锁没有被占用，直接进行CAS操作获取锁，将线程ID替换成自己的线程ID。如果锁状态不是0，代表有线程在访问该方法。此时，如果线程ID是自己的线程ID，如果是可重入锁，会将status自增1，然后获取到该锁，进而执行相应的方法。如果是非重入锁，就会进入阻塞队列等待。
释放锁时，可重入锁，每一次退出方法，就会将status减1，直至status的值为0，最后释放该锁。
释放锁时，非可重入锁，线程退出方法，直接就会释放该锁。





Synchronized应用举例：生产者消费者模型。利用缓冲区——管程法

消费者线程需要等待直到生产者线程完成一次写入操作。生产者线程需要等待消费者线程完成一次读取操作。假设没有应用Synchronized关键字，当消费者线程执行wait操作的同时，生产线线程执行notify，生产者线程可能在等待队列中找不到消费者线程。导致消费者线程一直处于阻塞状态。那么这个模型就要失败了。所以必须要加Synchronized关键字。


wait(),notify(),notifyAll()必须要在Synchronized关键中使用。否则抛出IllegalMonitorStateException



### 8.死锁：

#### 8.1 产生死锁的四个必要条件：
1.互斥条件
一个资源每次只能被一个进程使用，即在一段时间内某资源仅为一个进程所使用。此时如果有其他进程请求该资源，则请求进程只能等待。
2.请求与保持条件
进程中已经保持了至少一个资源，但又提出了新的资源请求，而该资源已经被其他进程占有，此时请求进程被阻塞，但对自己已经获得资源保持不放。
3.不可剥夺条件
进程未使用完的资源在未使用完毕之前，不能被其他进程强行夺走，即只能由获得该资源的进程自己来释放。
4.循环等待条件
若干进程间形成首尾相接循环等待资源的关系。在发生死锁时必然存在一个进程等待队列{P1，P2，…,Pn},其中P1等待P2占有的资源，P2等待P3占有的资源，…，Pn等待P1占有的资源，形成一个进程等待环路，环路中每一个进程所占有的资源同时被另一个申请。

注意：这四个条件是死锁的必然条件，只要系统发生死锁，这些条件必然成立。只要有上述条件有一条不满足，就不会发生死锁。

#### 8.2 死锁的预防：死锁的预防是设法至少破坏产生死锁的四个必要条件之一
我们可以通过破坏产生死锁的四个必要条件来预防死锁，由于资源互斥是固有特性无法改变的。
1.破坏“请求与保持”条件
方法一：静态分配，每个进程在开始执行时就申请他所需要的全部资源。
方法二：动态分配，每个进程在申请所需要的资源时他本身不占用系统资源。
2.破坏“不可剥夺”条件
一个进程不可获得其所需要的全部资源便处于等待状态，等待期间他占用的资源将被隐式的释放重新加入到系统的资源列表中，可以被其他进程使用，而等待的进程只有重新获得自己原有的资源以及新申请的资源才可以重新启动，执行。
3.破坏“循环等待”条件
采用资源有序分配的基本思想。将系统中的资源顺序进行编号，将紧缺的、稀少的资源采用较大的编号，申请资源时必须按照编号的顺序执行，一个进程只有较小编号的进程才能申请较大编号的进程。

#### 8.3 死锁的避免：银行家算法

银行家算法的基本思想是：分配资源之前，判断系统是否安全，如果安全才会进行资源分配。
每一个线程进入系统时，它必须声明在运行过程中，所需的每种资源类型最大数目，其数目不应超过系统所拥有每种资源总量，当线程请求一组资源系统必须确定有足够资源分配给该进程，若有在进一步计算这些资源分配给进程后，是否会使系统处于不安全状态，不会（即若能在分配资源时找到一个安全序列），则将资源分配给它，否则等待。

即：系统资源大于请求，则安全





# 操作系统：

## 高频考点

### 1.进程与线程的区别

进程：**系统进行资源调度和分配的最小独立单位**。具有独立性，动态性，并发性，异步性。实现了os的并发性

线程：**程序执行的最小单位**。自身基本不拥有系统资源，只拥有一些运行中必不可少的资源，[如程]()序计数器、寄存器和栈等。可与同属于一个进程的不同线程共享进程所拥有的全部资源。实现了进程间的并发性

联系：进程（主线程）创建了多个线程，各个子线程拥有自己的独立栈空间（存储函数参数、局部变量等），多个子线程与主线程共享**堆、全局变量**等非栈内存。一个程序至少有一个进程，一个进程至少有一个线程，线程依赖于进程而存在。

区别：**资源、切换效率、通信机制**

**一个线程挂掉，会导致该线程所属的进程整个挂掉，进程中的其他线程也都挂掉，但是一个进程挂掉，不会影响其他进程**

### 2.进程间的通信方式

#### 2.1 管道

#### 2.2 消息队列

#### 2.3 信号

#### 2.4 共享内存

### 3.线程间的通信方式

针对于python中的threading模块来说
3.1 全局变量
3.2 消息队列 （threading模块中的Queue类）

### 4.常见锁

4.1 互斥锁
4.2 多重入锁（允许一个进程/线程多次拿到锁）
4.3 自旋锁 （CPU不断检查锁是否可用）
4.4 事件
4.5 条件
4.6 信号量 （一次允许多个线程操作锁对象）
4.7 读写锁 （一次只有一个写者，多个读者）

### 5.死锁产生的原因及解决策略

#### 5.1 死锁产生的四个必要条件

1.互斥条件
2.占有并等待条件
3.环路等待条件
4.不抢占条件

#### 5.2 死锁解决策略

#### 1.死锁预防

主要的思想：提前破坏死锁产生的四个必要条件，静态地避免死锁发生

##### ①破坏互斥条件

把某些互斥访问的资源改造成共享资源（SPOOLing技术）

缺点：可行性不高

##### ②破坏占有并等待条件

方法一：在进程执行前，一次性分配进程所需的全部资源，如果资源得不到满足，则不分配任何资源，暂不执行。

方法二：只有当进程不占有资源时才分配给进程资源。进程可以占有一部分资源，但是当它向os索取更多资源的时候必须先释放当前占有的全部资源

缺点：进程动态执行，难以事先预知进程所需的全部资源；资源利用率低；可能会导致饥饿

##### ③破坏不抢占条件

方法一：申请的资源得不到满足时则立即释放当前所拥有的全部资源

方法二：由操作系统干预按照优先级从别的进程那里剥夺某些资源

缺点：实现复杂；强行剥夺可能会导致进程失效；反复申请和释放导致系统性能较低；可能导致饥饿

##### ④破坏环路条件

给**所有资源[排序]()编号**，所有进程对资源的申请必须按照**严格递增**的顺序提出，每次只能申请序号更大的资源，防止产生环路

缺点：导致资源浪费，不方便加入新资源

#### 2.死锁避免

**动态地检测资源分配状态**，以确保系统处于安全状态。

通过银行家[算法]()进行动态评估，如果有风险就拒绝分配

> **银行家[算法]()**
>
> 步骤：
>
> 1.一个进程向系统提出资源申请，首先检查此次申请是否超过了之前声明的最大需求数，如果超过则认为出错；
>
> 2.检查此时系统剩余的可用资源是否还能够满足该次请求，若不满足则等待
>
> 3.试探着（不是真的分配）分配，更改各类数据结构
>
> 4.用安全性[算法]()检查此次资源分配是否会导致系统进入不安全状态，若安全才进行资源分配
>
> 总结：进程提出资源请求时，判断是否超出进程最大需求资源、是否超出系统最大所剩资源，然后模拟分配，用安全[算法]()检测分配了之后是否所有线程属于安全序列，如果安全则才能正式分配资源。
>
> > **安全[算法]()**
> >
> > 检查当前的剩余可用资源是否能满足某个进程的最大需求，如果可以，就将该进程加入到安全序列，并把该进程持有的资源全部回收，重复上述过程，看最终是否能够让所有进程加入安全序列。
>
> **系统处于安全状态一定不会死锁**
>
> **系统处于不安全状态未必会死锁，但是死锁时系统一定是不安全状态**

#### 3.死锁检测

采用资源分配图[算法]()检测最终哪些进程会发生死锁。（允许发生死锁，主要是检测哪些进程会发生死锁）
![图片说明](https://uploadfiles.nowcoder.com/images/20210330/972694929_1617118905062/4CB5CC901D8D1EDCF9687E43DF8E2E89) 

> 流程：消除所有不与阻塞进程相连的边，直到无边可消。如果该节点的资源需求得到满足，则消除该节点及其所连的边，直到无边可消。如果此时图内没有任何边，则一定没有死锁，如果还有则一定会发生死锁。

#### 4.死锁解除

##### ①资源剥夺法

挂起某些死锁资源，抢占其资源分配给其他的死锁进程

缺点：被挂起的进程可能会导致饥饿

##### ②终止进程法

强制终止部分、甚至全部死锁进程，并剥夺其所占有的资源

缺点：付出代价较大，有些进程已近结束，终止就会功亏一篑

##### ③进程回退法

让一个或多个进程回退到不会发生死锁的地步

缺点：要求系统记录进程的历史信息，设置还原点，浪费系统资源

## 常见考点

### 1.内存换出[算法]()

**FIFO先进先出** 

思想：淘汰最先进入的页面，采用队列实现，先进先出

缺点：可能会频繁地换入换出，影响效率

**OPT最佳[算法]()**

思想：淘汰以后不需要使用或者最远才会用到的页

缺点：实际操作中无法预测未来页的使用情况

**LRU最久未使用淘汰[算法]()**

思想：淘汰最长时间没有被使用的页。软件可以用**双向[链表]()**实现，访问的时候就把该页移到头部，淘汰时淘汰尾部。

**LFU最不经常使用淘汰[算法]()**

思想：淘汰访问频率最小的页，以次数为参考。新加入的页放在末尾，计数器置为1，每次访问计数器+1，并重新按照计数器大小[排序]()，淘汰计数器最小的页

### 2.进程调度[算法]()

1.先来先服务
2.短作业优先
3.高响应比优先
4.时间片轮转法
5.优先级调度[算法]()
6.多级反馈队列
 分为多个队列，**优先级从高到低**，不同的队列**分配的时间片由小到大**

 进程到来先将其加入第一队列，按照FCFS的原则给第一队列的进程分配时间片，时间片耗尽进程未处理完毕则将其加入到下一队列的尾部（本身就是最后一个队列的话就加入到最后一个队列的尾部）。只有当1~n-1队列为空时才能开始处理第n个队列。

**实际情况中如何选择进程调度[算法]()？**

根据是否要关注优先权，如果不考虑优先权可以采用高响应比优先[算法]()，如果考虑优先权可以采用多级反馈队列。

